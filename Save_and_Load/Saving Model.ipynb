{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9175c6",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f60bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 09:47:25.192081: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-20 09:47:25.192098: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab02b88",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ffe88",
   "metadata": {},
   "source": [
    "Use `with_info=True` to get the metadata about the dataset and `as_supervised=True` to receive the corresponding labels on the dataset. The `tf_flowers` dataset does not include a test set, so our only options are from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc441802",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_set, validation_set), dataset_info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc731b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set:2936, and length of validation set:734\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of training set:{len(training_set)}, and length of validation set:{len(validation_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9249b03",
   "metadata": {},
   "source": [
    "## Resizing the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df9929",
   "metadata": {},
   "source": [
    "We need to resize the images as our model expects images all of the same size. The \".resize\" method Resizes `images` to `size` using the specified `method`. Then we normalize all the pixel values as all tensorflorhub models expect tensors to be in the range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a54c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image(image, label):\n",
    "    image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72353aee",
   "metadata": {},
   "source": [
    "For this model we will be transferring over the MobileNet model which expects images of size 224 x 224. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea14662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = dataset_info.splits['train'].num_examples\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_RES = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c730096",
   "metadata": {},
   "source": [
    "### Input Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7733c7",
   "metadata": {},
   "source": [
    "Because we don't care about the order in which the images are resized we can do them \"in parallel\" and `AUTOTUNE` decides the number of images done in parallel. We `.cache()` the batches to speed up future execution of the pre-processing. The shuffle buffer size just ensures the shuffle the data randomly. The `.prefetch(AUTOTUNE)` will prefetch 32 examples instantly after the previous GPU calls are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717b377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow deciding AUTOTUNE value\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_batches = training_set.cache().shuffle(num_examples//4).map(format_image,num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "validation_batches = validation_set.cache().map(format_image,num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b571f",
   "metadata": {},
   "source": [
    "# Transfer Learning with MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6473b34",
   "metadata": {},
   "source": [
    "First get the URL from TensorFlowHub for the specific model we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533fc796",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "\n",
    "feauture_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e3aed",
   "metadata": {},
   "source": [
    "Freeze all the variables within the feature_extractor layer so that during training none of the parameters are changed within these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5920ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feauture_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9ba68",
   "metadata": {},
   "source": [
    "## Attaching a Classification Final Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c78e61fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of different classes in the flowers dataset\n",
    "classes_num = dataset_info.features['label'].num_classes\n",
    "classes_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aaa3fc",
   "metadata": {},
   "source": [
    "We need to wrap the hub layer with a tf.keras.Sequential model and add the new classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a47dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 6405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feauture_extractor,\n",
    "    layers.Dense(classes_num, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b1b55",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ced876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "92/92 [==============================] - 33s 338ms/step - loss: 0.6973 - accuracy: 0.7483 - val_loss: 0.4184 - val_accuracy: 0.8638\n",
      "Epoch 2/5\n",
      "92/92 [==============================] - 29s 319ms/step - loss: 0.3640 - accuracy: 0.8770 - val_loss: 0.3386 - val_accuracy: 0.8896\n",
      "Epoch 3/5\n",
      "92/92 [==============================] - 23s 255ms/step - loss: 0.2899 - accuracy: 0.9040 - val_loss: 0.2983 - val_accuracy: 0.9114\n",
      "Epoch 4/5\n",
      "92/92 [==============================] - 22s 238ms/step - loss: 0.2379 - accuracy: 0.9268 - val_loss: 0.2754 - val_accuracy: 0.9169\n",
      "Epoch 5/5\n",
      "92/92 [==============================] - 22s 243ms/step - loss: 0.2049 - accuracy: 0.9435 - val_loss: 0.2717 - val_accuracy: 0.9183\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 5\n",
    "history = model.fit(train_batches,\n",
    "                   epochs=EPOCHS,\n",
    "                   validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813adc7",
   "metadata": {},
   "source": [
    "## Checking Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a88edca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = np.array(dataset_info.features['label'].names)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f6747",
   "metadata": {},
   "source": [
    "Run an image batch through the model and convert the indices to class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0ce0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 215ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['tulips', 'tulips', 'sunflowers', 'dandelion', 'sunflowers',\n",
       "       'roses', 'daisy', 'roses', 'dandelion', 'daisy', 'daisy',\n",
       "       'sunflowers', 'dandelion', 'dandelion', 'dandelion', 'sunflowers',\n",
       "       'daisy', 'dandelion', 'dandelion', 'roses', 'tulips', 'sunflowers',\n",
       "       'dandelion', 'dandelion', 'daisy', 'roses', 'sunflowers', 'daisy',\n",
       "       'daisy', 'sunflowers', 'tulips', 'daisy'], dtype='<U10')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_batches.take(1)))\n",
    "image_batch = image_batch.numpy()\n",
    "label_batch = label_batch.numpy()\n",
    "\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch).numpy()\n",
    "\n",
    "predicted_ids = np.argmax(predicted_batch, axis=-1)\n",
    "predicted_class_names = class_names[predicted_ids]\n",
    "\n",
    "predicted_class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c2e19",
   "metadata": {},
   "source": [
    "Now compare with the true labels and predicted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80801cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:            [2 2 3 0 3 4 1 2 0 1 1 3 0 0 0 3 1 0 0 4 2 3 0 0 1 2 3 1 1 3 2 1]\n",
      "Predicted Labels:  [2 2 3 0 3 4 1 4 0 1 1 3 0 0 0 3 1 0 0 4 2 3 0 0 1 4 3 1 1 3 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted Labels: \",predicted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a372b",
   "metadata": {},
   "source": [
    "# Saving Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416102b",
   "metadata": {},
   "source": [
    "Once the model has been trained, we can save the model as a HDF5 file which is the format used by Keras, and this file will have the extension `.h5` and its name will correspond to the current time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06eed51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./1658338888.h5\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"./{}.h5\".format(int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f707db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 alex alex 9.0M Jul 20 10:41 1658338888.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh | grep .h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8d398",
   "metadata": {},
   "source": [
    "# Export as SavedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a58bc",
   "metadata": {},
   "source": [
    "Another way to save models is to use the TensorFlow SavedModel format which is a standalone serialization format for Tensorflow objects. It contains a complete TensorFlow program, including weights and computations and it doesn't require the original model building code to run.\n",
    "\n",
    "The SavedModel files that are created:\n",
    "1. tf checkpoint containing model weights\n",
    "2. A SavedModel proto containing the underlying tf graph. Separate graphs are saved for predictions (serving), train, and evaluation. If the model wasn't compiled before, then only the inference graph gets exported\n",
    "3. The model's architecture config, if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212b350",
   "metadata": {},
   "source": [
    "The `tf.saved_model.save` method takes in the original model we desire to save and the path to the folder where we want to save the model. This function will create a folder where you'll find an `assets` folder, a `variables` folder, and the `saved_model.pb` folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
